---
title: "IQ and other relations with the body"
output:
  pdf_document:
    latex_engine: xelatex
  html_notebook: default
  html_document: default
---

## Section A

Does the PIQ depend of other measures?
```{r}
df <- read.csv("dfact4.csv")
head(df)
```
Lets do a multiple linear regression. We can use the lm function and predict the variable PIQ dependind on the others:
```{r}
df.lm <- lm(PIQ~Brain + Height + Weight,df)
df.lm
summary(df.lm)
```
We see that is mostly non-related with weight but the brain and height has some correlation with it.

Nevertheless, even if height seems to have a really low $\mathit{p}$-value, we see that for our dataset, the brain
is correlated positively with a coefficient around 2 and a standard error around 1/2.

On the other hand, Height has a negative correlation of -2.732 and a standard error of 1.229 which is really high variation.
This, together with his high variation makes it a less reliable source for the PIQ than the Brain variable.

Furthermore, the adjusted R^2 is really low so this means the phenomena is not explained well through the predictors.
The standard error is aproximately 19.79 and the variationis 22.59 so there´s a humongous variation between the predictions.

As we see in the valur "Multiple R-Squared" only about 29% of variables are explained through brain length.
```{r}
coefficients(df.lm)
```

- Which of the predictors explain something about the variability of the intellectual coefficient?

Mainly the brain, as it is correlated positively and has a strong lower $\mathit{p}$-value that backs up the hypothesis that for bigger brains we will have more IQ.

- Which is the effect on cerebral length, after taking in consideration the weight and height?

We can introduce a polinomical factor to increase the accuracy of the model in the variable "Brain":

```{r}
df.exp_model <- lm(PIQ~Brain + I(Brain^3) + Height + Weight,df)
summary(df.exp_model)

```
As we see we have included Brain^3 as another variable, and the Adjusted R-squared has gotten worse so the explanation capability has worsen.

- Give me a confidence interval for the coefficients.

We delete the variables that are not relevant (weight) and we calculate the confidenece intervals:
```{r}
df.lm3 <- lm(PIQ ~Brain + Height, df)
summary(df.lm3)
confint(df.lm3)
```
Using the function "$\mathit{confint}$" we obtain that the confidence intervals are:

$Intercept: [-2.14,224,69]$
$Brain: [0.95,3.17]$
$Height: [-4.74,-0.71]$

- Which is the IQ confidence interval for the predictor values: Brain length = 95, Height = 70, Weight = 180?

We do not consider inn our equation the weiht of the person as it worsen the model´s accuracy.
```{r}
new_data <- data.frame(Brain = 95, Height = 70)
predict(df.lm3,new_data, interval = "predict") 
#If we want to make predictions, we use interval = "Predict"
#Similar to conf intervals but wider and more predictive power for a single variable.
```

- Do residues follow a normal distro? Make the histogram representations and the convenient quantile-quantile graphics.
```{r}
df.lm3$residuals
par(mfrow=c(1,2))
hist(df.lm3$residuals)
qqnorm(df.lm3$residuals)
qqline(df.lm3$residuals)
```
How do we know if the residues follow a normal distribution?
We perform a Shapiro-Wilk test.

```{r}
shapiro.test(df.lm3$residuals)
```
As the $\mathit{p}$-value is more than 0.05, we accept $H_{0}$ and it follows a normal distribution.


## Section B

- Load `prostate.csv`
```{r}
df2 <- read.csv("prostate.csv")
head(df2)
```

- Delete the train column
```{r}
df2$train <- NULL
df2
```

- which is the best model that explains "lqsa"?

To pick the best model, we use the algorithm AIC together with the "stepwise" criteria so we evaluate the models inside the limits that we define.

- Lower limit = Constant function.
- Upper limit = Model with all the variables.

We first apply the algorithm with the backwards case.

```{r}
fit1 <- lm(lpsa~lcavol+lweight+age+lbph+svi+lcp+gleason+pgg45,data=df2)
fit0 <- lm(lpsa~1,data=df2)
library(MASS)
stepAIC(fit1,direction="backward")
```

Now we execute it "forward":
```{r}
stepAIC(fit0,direction="forward",scope=list(upper=fit1,lower=fit0))
```
Finally we do it with "both" directions:

```{r}
step <- stepAIC(fit0,direction="both",scope=list(upper=fit1,lower=fit0))
step$call
```

So the model is based lineally with "lcavol", "lweight", "svi", "lbph" and "age".
We make a summary of the model:
```{r}
summary(lm(formula = lpsa ~ lcavol + lweight + svi + lbph + age, data = df2))
coef(step)
anova(lm(formula = lpsa ~ lcavol + lweight + svi + lbph, data = df2),
      lm(formula = lpsa ~ lcavol + lweight + svi + lbph + age, data = df2))
```
We see here that p-value is 0.12 so we will keep the null hypothesis that is the one withour "Age". Nevertheless, when we perform the "extractAIC" we find that:
```{r}
extractAIC(lm(formula = lpsa ~ lcavol + lweight + svi + lbph , data = df2),scale=0)
```
It is better than the other models than using the Stepwise. 

Notice that the method of AIC is better as it is a measure that is objective about the performance while the other one is not a white/black answer, as the nearer to the p-value, the more we doubt about the results.

Shape of the residues:
```{r}
par(mfrow=c(1,2))
hist(step$residuals)
qqnorm(step$residuals)
qqline(step$residuals)
```
- Confidence interval for the coefficients.

```{r}
coefficients(step) # coeficientes del modelo
confint(step) # intervalos de confianza para cada coeficiente
```
